{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"   ![](https://s30876.pcdn.co/wp-content/uploads/Why-Did-You-Leave-Your-Last-Job-1170x630.jpg)","metadata":{}},{"cell_type":"markdown","source":"## <center><span style=\"color:red\"> Upvote my work if you like it </span></center>","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n\n* [Approaching Categorical Features](#1)\n* [Various Approaches to Handle Missing values in Categorical Features](#2)\n* [k-Nearest Neighbour Imputation](#3)\n* [Evaluation Metrics](#4)\n* [Model](#5)\n* [Learning Curve](#6)\n* [Oversampling using SMOTE](#7)\n* [Hyperparameter Tunning](#8)\n* [Reference](#9)","metadata":{}},{"cell_type":"markdown","source":"## Import important libraries and packages","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix, log_loss, plot_roc_curve, auc, precision_recall_curve\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n%matplotlib inline\nfrom xgboost import XGBClassifier\nfrom functools import partial\nfrom skopt import gp_minimize\nfrom skopt import space\nfrom skopt.plots import plot_convergence\n\nsns.set_style('whitegrid')\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-12T21:01:23.823740Z","iopub.execute_input":"2022-04-12T21:01:23.824155Z","iopub.status.idle":"2022-04-12T21:01:23.851522Z","shell.execute_reply.started":"2022-04-12T21:01:23.824122Z","shell.execute_reply":"2022-04-12T21:01:23.850265Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ndf_test = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-04-12T21:01:23.861446Z","iopub.execute_input":"2022-04-12T21:01:23.862217Z","iopub.status.idle":"2022-04-12T21:01:23.944532Z","shell.execute_reply.started":"2022-04-12T21:01:23.862145Z","shell.execute_reply":"2022-04-12T21:01:23.942932Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:23.946960Z","iopub.execute_input":"2022-04-12T21:01:23.947426Z","iopub.status.idle":"2022-04-12T21:01:23.976121Z","shell.execute_reply.started":"2022-04-12T21:01:23.947377Z","shell.execute_reply":"2022-04-12T21:01:23.974808Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:23.978356Z","iopub.execute_input":"2022-04-12T21:01:23.978689Z","iopub.status.idle":"2022-04-12T21:01:24.000024Z","shell.execute_reply.started":"2022-04-12T21:01:23.978642Z","shell.execute_reply":"2022-04-12T21:01:23.998731Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"Features\n\n* enrollee_id : Unique ID for candidate\n* city: City code\n* city_ development _index : Developement index of the city (scaled)\n* gender: Gender of candidate\n* relevent_experience: Relevant experience of candidate\n* enrolled_university: Type of University course enrolled if any\n* education_level: Education level of candidate\n* major_discipline :Education major discipline of candidate\n* experience: Candidate total experience in years\n* company_size: No of employees in current employer's company\n* company_type : Type of current employer\n* lastnewjob: Difference in years between previous job and current job\n* training_hours: training hours completed\n* target: 0 – Not looking for job change, 1 – Looking for a job change","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.011071Z","iopub.execute_input":"2022-04-12T21:01:24.011460Z","iopub.status.idle":"2022-04-12T21:01:24.046478Z","shell.execute_reply.started":"2022-04-12T21:01:24.011429Z","shell.execute_reply":"2022-04-12T21:01:24.045001Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.048692Z","iopub.execute_input":"2022-04-12T21:01:24.049044Z","iopub.status.idle":"2022-04-12T21:01:24.067907Z","shell.execute_reply.started":"2022-04-12T21:01:24.049012Z","shell.execute_reply":"2022-04-12T21:01:24.066386Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"# Approaching Categorical Features<a id = \"1\" ></a>\n\n\nCategorical variables/features are any feature type can be classified into two major\ntypes:\n*  Nominal\n*  Ordinal\n\n**Nominal variables** are variables that have two or more categories which do not\nhave any kind of order associated with them. For example, if gender is classified\ninto two groups, i.e. male and female, it can be considered as a nominal variable.\n\n**Ordinal variables** on the other hand, have “levels” or categories with a particular\norder associated with them. For example, an ordinal categorical variable can be a\nfeature with three different levels: low, medium and high. Order is important.","metadata":{}},{"cell_type":"markdown","source":"**List of ordinal variables in this data**\n\n1. education_level\n2. company_size\n3. experience\n4. last_new_job\n5. company_type","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.082305Z","iopub.execute_input":"2022-04-12T21:01:24.082654Z","iopub.status.idle":"2022-04-12T21:01:24.107653Z","shell.execute_reply.started":"2022-04-12T21:01:24.082624Z","shell.execute_reply":"2022-04-12T21:01:24.106048Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"## Various Approaches to Encode Categorical Features\n","metadata":{}},{"cell_type":"code","source":"#lets combine train and test sets to preprocess the data\n\n#First i suggest to create a fake target feature in test set with some same value for every single element\n#By this it will be easy for us to combine and seprate our training and test data after data preprocessing\n\n#can plot count plot for more intution\n\ndf_test['target'] = -1 #remeber that we have to drop this column later\n\ndf_pre = pd.concat([df_train, df_test], axis = 0).reset_index(drop = True)\n# Just a Tip always reset the indices whenever you join or disjoin two or more datasets","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.113636Z","iopub.execute_input":"2022-04-12T21:01:24.114079Z","iopub.status.idle":"2022-04-12T21:01:24.133521Z","shell.execute_reply.started":"2022-04-12T21:01:24.114044Z","shell.execute_reply":"2022-04-12T21:01:24.132103Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"df_pre.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.148942Z","iopub.execute_input":"2022-04-12T21:01:24.149336Z","iopub.status.idle":"2022-04-12T21:01:24.188835Z","shell.execute_reply.started":"2022-04-12T21:01:24.149303Z","shell.execute_reply":"2022-04-12T21:01:24.187452Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"\n**Label Encoding** refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning.\n![](https://ekababisong.org/assets/seminar_IEEE/LabelEncoder.png)  \n\nWe can do label Encoding From LabelEncoder of scikit-Learn but to do so first we have to impute missing values in data ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n#Making Copy of data just for example\ndf_lb = df_pre.copy()\ndf_lb['education_level'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.190896Z","iopub.execute_input":"2022-04-12T21:01:24.191203Z","iopub.status.idle":"2022-04-12T21:01:24.208504Z","shell.execute_reply.started":"2022-04-12T21:01:24.191175Z","shell.execute_reply":"2022-04-12T21:01:24.207230Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"The feature column education_level of data is in categorical form as we can see above output","metadata":{}},{"cell_type":"code","source":"#Fill nan values\ndf_lb.loc[:, \"education_level\"] = df_lb['education_level'].fillna(\"NONE\")","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.214987Z","iopub.execute_input":"2022-04-12T21:01:24.215323Z","iopub.status.idle":"2022-04-12T21:01:24.224261Z","shell.execute_reply.started":"2022-04-12T21:01:24.215294Z","shell.execute_reply":"2022-04-12T21:01:24.223145Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"In above code cell i just create null values as new category \"NONE\"","metadata":{}},{"cell_type":"code","source":"# initialize LabelEncoder\nlbl_enc = LabelEncoder()\n\n# fit label encoder and transform values on ord_2 column\ndf_lb.loc[:, \"education_level\"] = lbl_enc.fit_transform(df_lb['education_level'].values)\n\ndf_lb['education_level'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.249883Z","iopub.execute_input":"2022-04-12T21:01:24.250215Z","iopub.status.idle":"2022-04-12T21:01:24.271589Z","shell.execute_reply.started":"2022-04-12T21:01:24.250186Z","shell.execute_reply":"2022-04-12T21:01:24.270502Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"The feature column education_level of data is now transformed into numarical form as we can see above\n\nBut in this Notebook i am not going to use scikit-Learn LabelEncoder Due to following reasons\n\n1. Label Encoder encode data on basis of count but as mentioned above this data have lots of ordinal features means categories of some features might depend      upon some levels like in education_level feature\\ \n\n   We know that we should encode data in this order but label encoder encodes it on basis of count\n   \n      Primary School  \n      High School            \n      Graduate          \n      Masters           \n      Phd\n    \n2. To use label encoder first we have to create NULL values as new category and Our data have so many NULL values so we can not just create new Category for      NULL values because due to this data distribution could change\n\n","metadata":{}},{"cell_type":"code","source":"# Making Dictionaries of ordinal features\n\ngender_map = {\n        'Female': 2,\n        'Male': 1,\n        'Other': 0\n         }\n\nrelevent_experience_map = {\n    'Has relevent experience':  1,\n    'No relevent experience':    0\n}\n\nenrolled_university_map = {\n    'no_enrollment'   :  0,\n    'Full time course':    1, \n    'Part time course':    2 \n}\n    \neducation_level_map = {\n    'Primary School' :    0,\n    'Graduate'       :    2,\n    'Masters'        :    3, \n    'High School'    :    1, \n    'Phd'            :    4\n    } \n    \nmajor_map ={ \n    'STEM'                   :    0,\n    'Business Degree'        :    1, \n    'Arts'                   :    2, \n    'Humanities'             :    3, \n    'No Major'               :    4, \n    'Other'                  :    5 \n}\n    \nexperience_map = {\n    '<1'      :    0,\n    '1'       :    1, \n    '2'       :    2, \n    '3'       :    3, \n    '4'       :    4, \n    '5'       :    5,\n    '6'       :    6,\n    '7'       :    7,\n    '8'       :    8, \n    '9'       :    9, \n    '10'      :    10, \n    '11'      :    11,\n    '12'      :    12,\n    '13'      :    13, \n    '14'      :    14, \n    '15'      :    15, \n    '16'      :    16,\n    '17'      :    17,\n    '18'      :    18,\n    '19'      :    19, \n    '20'      :    20, \n    '>20'     :    21\n} \n    \ncompany_type_map = {\n    'Pvt Ltd'               :    0,\n    'Funded Startup'        :    1, \n    'Early Stage Startup'   :    2, \n    'Other'                 :    3, \n    'Public Sector'         :    4, \n    'NGO'                   :    5\n}\n\ncompany_size_map = {\n    '<10'          :    0,\n    '10/49'        :    1, \n    '100-500'      :    2, \n    '1000-4999'    :    3, \n    '10000+'       :    4, \n    '50-99'        :    5, \n    '500-999'      :    6, \n    '5000-9999'    :    7\n}\n    \nlast_new_job_map = {\n    'never'        :    0,\n    '1'            :    1, \n    '2'            :    2, \n    '3'            :    3, \n    '4'            :    4, \n    '>4'           :    5\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.368850Z","iopub.execute_input":"2022-04-12T21:01:24.369206Z","iopub.status.idle":"2022-04-12T21:01:24.385619Z","shell.execute_reply.started":"2022-04-12T21:01:24.369177Z","shell.execute_reply":"2022-04-12T21:01:24.384384Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"I am using mapping to transform categorical features into numarical features","metadata":{}},{"cell_type":"code","source":"# Transforming Categorical features into numarical features\n\ndf_pre.loc[:,'education_level'] = df_pre['education_level'].map(education_level_map)\ndf_pre.loc[:,'company_size'] = df_pre['company_size'].map(company_size_map)\ndf_pre.loc[:,'company_type'] = df_pre['company_type'].map(company_type_map)\ndf_pre.loc[:,'last_new_job'] = df_pre['last_new_job'].map(last_new_job_map)\ndf_pre.loc[:,'major_discipline'] = df_pre['major_discipline'].map(major_map)\ndf_pre.loc[:,'enrolled_university'] = df_pre['enrolled_university'].map(enrolled_university_map)\ndf_pre.loc[:,'relevent_experience'] = df_pre['relevent_experience'].map(relevent_experience_map)\ndf_pre.loc[:,'gender'] = df_pre['gender'].map(gender_map)\ndf_pre.loc[:,'experience'] = df_pre['experience'].map(experience_map)\n\n#encoding city feature using label encoder\nlb_en = LabelEncoder()\n\ndf_pre.loc[:,'city'] = lb_en.fit_transform(df_pre.loc[:,'city']) \n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.387998Z","iopub.execute_input":"2022-04-12T21:01:24.388405Z","iopub.status.idle":"2022-04-12T21:01:24.482608Z","shell.execute_reply.started":"2022-04-12T21:01:24.388367Z","shell.execute_reply":"2022-04-12T21:01:24.481507Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"df_pre.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.484923Z","iopub.execute_input":"2022-04-12T21:01:24.485301Z","iopub.status.idle":"2022-04-12T21:01:24.507195Z","shell.execute_reply.started":"2022-04-12T21:01:24.485262Z","shell.execute_reply":"2022-04-12T21:01:24.506382Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"df_pre.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.536477Z","iopub.execute_input":"2022-04-12T21:01:24.537122Z","iopub.status.idle":"2022-04-12T21:01:24.552403Z","shell.execute_reply.started":"2022-04-12T21:01:24.537074Z","shell.execute_reply":"2022-04-12T21:01:24.551192Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"We can use this directly in many tree-based models like:\n*  Decision trees\n*  Random forest\n*  Extra Trees\n*  Or any kind of boosted trees model\n\n   * XGBoost\n   * GBM\n   * LightGBM\n   \nGenerally, in tree-based models the scale of the features does not matter. This is because at each tree level, the score of a possible split will be equal whether the respective feature has been scaled or not.\n\nYou can think of it like here: We're dealing with a binary classification problem and the feature we're splitting takes values from 0 to 1000. If you split it on 300, the samples <300 belong 90% to one category while those >300 belong 30% to one category. Now imaging this feature is scaled between 0 and 1. Again, if you split on 0.3, the sample <0.3 belong 90% to one category while those >0.3 belong 30% to one category.\n\nSo you've changed the splitting point but the actual distribution of the samples remains the same regarding the target variable.\n\nAbove example is taken from : [Why Normalization is not required for tree based models](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/160613)\n\n\nThis type of encoding cannot be used in linear models, support vector machines or neural networks as they expect data to be normalized (or standardized). For these types of models, we can binarize the data.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Approch For Categorical Features(Summary)\n\n1. Fill nan values with some value like NONE to which we can deal later \n2. Then convert categorical features in numarical using label encoder\n\n**NOTE : If you want to use some tree based algorithm than these much steps are sufficent**\n\n3. In case of non tree based model do One Hot Encoding of numarical feature acieved from step 2 and make sparse = True in      One Hot Encoding\n4. Now select model and train your data","metadata":{}},{"cell_type":"markdown","source":"# Various Approaches to Handle Missing values in Categorical Features <a id = \"2\" ></a>\n\n1. You can simply drop columns having very large amount of null values\n2. Drop entire row if it has some null values (But this approach is **not Recommendable** because then we will lose lots of precious data)\n3. First convert none null categorical data into numarical form and then simply fill mean, mode or median value inplace of null values\n4. First convert none null categorical data into numarical form and then with the help of k-Nearest Neighbors algorithm find missing values and impute them in    data\n5. Another way of imputing missing values in a column would be to train a regression model that tries to predict missing values in a column based on other        columns.\n\n**In This notebook i am using k-Nearest Neighbors algorithm to fill missing values**\n","metadata":{}},{"cell_type":"markdown","source":"Below i am plotting count of values per columns of dataset. I also have sorted columns based on missing values.","metadata":{}},{"cell_type":"code","source":"missingno.bar(df_pre,color=\"dodgerblue\", sort=\"ascending\", figsize=(10,5), fontsize=12);","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:24.576468Z","iopub.execute_input":"2022-04-12T21:01:24.576838Z","iopub.status.idle":"2022-04-12T21:01:25.372601Z","shell.execute_reply.started":"2022-04-12T21:01:24.576806Z","shell.execute_reply":"2022-04-12T21:01:25.371785Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"#Just to check number of null values of every column in data\n\nfor col in df_pre.columns:\n    null_val = df_pre[col].isnull().sum()\n    null_prec = (null_val * 100) / df_pre.shape[0]\n    print('> %s , Missing: %d (%.1f%%)' % (col, null_val, null_prec))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:25.374039Z","iopub.execute_input":"2022-04-12T21:01:25.374455Z","iopub.status.idle":"2022-04-12T21:01:25.392539Z","shell.execute_reply.started":"2022-04-12T21:01:25.374425Z","shell.execute_reply":"2022-04-12T21:01:25.391476Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"Below we are plotting heatmap showing nullity correlation between various columns of dataset.\n\nThe nullity correlation ranges from -1 to 1.\n\n* -1 - Exact Negative correlation represents that if the value of one variable is present then the value of other variables is definitely absent.\n* 0 - No correlation represents that variables values present or absent do not have any effect on one another.\n* 1 - Exact Positive correlation represents that if the value of one variable is present then the value of the other is definitely present.","metadata":{}},{"cell_type":"code","source":"missingno.heatmap(df_pre, cmap=\"RdYlGn\", figsize=(10,5), fontsize=12);","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:25.394598Z","iopub.execute_input":"2022-04-12T21:01:25.395050Z","iopub.status.idle":"2022-04-12T21:01:25.788643Z","shell.execute_reply.started":"2022-04-12T21:01:25.395006Z","shell.execute_reply":"2022-04-12T21:01:25.787633Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# <center> k-Nearest Neighbour Imputation <a id = \"3\" ></a> </center>\n\n\nA fancy way of filling in the missing values would be to use a **k-nearest neighbour** method. You can select a sample with missing values and find the nearest\nneighbours utilising some kind of distance metric, for example, Euclidean distance. Then you can take the mean of all nearest neighbours and fill up the missing value. You can use the KNN imputer implementation for filling missing values like this.\n\n![image1](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png)\n\n\n[K-Nearest Neighbors (KNN) Algorithm for Machine Learning](https://medium.com/capital-one-tech/k-nearest-neighbors-knn-algorithm-for-machine-learning-e883219c8f26)\n\n\n### How it works?\n\nStep-1: Select the K number of the neighbors\n        \nlet say we select K = 5\n        \n![image2](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning3.png) \n\n\nStep-2: Calculate the Euclidean distance of K number of neighbors\n\nIn this step we search for those k = 5 neighbors having minimum Euclidean Distance from unknown data point\n\n\n![Image4](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning4.png)\n\n        \nStep-3: Among these k neighbors, count the number of the data points in each category.\n\n![image3](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning5.png)\n\n\nStep-4: Assign the new data points to that category for which the number of the neighbor is maximum.\n\nYou can also visit below given youtube video link to understand it bit nicely\n\n[Step-by-Step procedure of KNN Imputer for imputing missing values](https://www.youtube.com/watch?v=AHBHMQyD75U)","metadata":{}},{"cell_type":"code","source":"df_pre1 = df_pre.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:25.790529Z","iopub.execute_input":"2022-04-12T21:01:25.790991Z","iopub.status.idle":"2022-04-12T21:01:25.797048Z","shell.execute_reply.started":"2022-04-12T21:01:25.790947Z","shell.execute_reply":"2022-04-12T21:01:25.795962Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"knn_imputer = KNNImputer(n_neighbors = 3)\n\nX = np.round(knn_imputer.fit_transform(df_pre1))\ndf_pre1 = pd.DataFrame(X, columns = df_pre1.columns)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:25.800836Z","iopub.execute_input":"2022-04-12T21:01:25.801172Z","iopub.status.idle":"2022-04-12T21:01:42.372515Z","shell.execute_reply.started":"2022-04-12T21:01:25.801141Z","shell.execute_reply":"2022-04-12T21:01:42.371272Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"df_pre1.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:42.376281Z","iopub.execute_input":"2022-04-12T21:01:42.376601Z","iopub.status.idle":"2022-04-12T21:01:42.390085Z","shell.execute_reply.started":"2022-04-12T21:01:42.376569Z","shell.execute_reply":"2022-04-12T21:01:42.388508Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"df_pre1.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:42.391806Z","iopub.execute_input":"2022-04-12T21:01:42.392216Z","iopub.status.idle":"2022-04-12T21:01:42.420706Z","shell.execute_reply.started":"2022-04-12T21:01:42.392173Z","shell.execute_reply":"2022-04-12T21:01:42.419658Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"But wait as you can observe city_development_index feature values were in continues form before imputation and after imputation now they are in discrete form.\nAs you may also noticed i was rounding values after imputation because of this city_development_index values converted into discrete form\n\nI am plotting kernel density estimate plot just to check distribution of city_development_index feature before and after imputation. You can plot KDE plots for every feature to check effect of imputaion on data","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,8))\nplt.title('Before Imputation')\ndf_pre['city_development_index'].plot(kind = 'kde')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:42.422171Z","iopub.execute_input":"2022-04-12T21:01:42.422759Z","iopub.status.idle":"2022-04-12T21:01:43.382784Z","shell.execute_reply.started":"2022-04-12T21:01:42.422708Z","shell.execute_reply":"2022-04-12T21:01:43.381538Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,8))\nplt.title('After Imputation')\ndf_pre1['city_development_index'].plot(kind = 'kde')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:43.384413Z","iopub.execute_input":"2022-04-12T21:01:43.384866Z","iopub.status.idle":"2022-04-12T21:01:44.212316Z","shell.execute_reply.started":"2022-04-12T21:01:43.384824Z","shell.execute_reply":"2022-04-12T21:01:44.211448Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"So rather than using imputation on whole dataset just use it on those features having missing values.","metadata":{}},{"cell_type":"code","source":"# missing columns\n\nmissing_cols = df_pre.columns[df_pre.isna().any()].tolist()\nmissing_cols","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:44.213469Z","iopub.execute_input":"2022-04-12T21:01:44.213905Z","iopub.status.idle":"2022-04-12T21:01:44.222313Z","shell.execute_reply.started":"2022-04-12T21:01:44.213862Z","shell.execute_reply":"2022-04-12T21:01:44.221307Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"Above shown columns have missing values and all 8 are categorical features\n\nNow i would like make two different dataframes one having features with missing values and second having features without missing values. But there will be one common column enrollee_id so that later we can perform inner join on both dataframes","metadata":{}},{"cell_type":"code","source":"#dataframe having features with missing values\ndf_missing = df_pre[['enrollee_id'] + missing_cols]\n\n#dataframe having features without missing values\ndf_non_missing = df_pre.drop(missing_cols, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:44.223351Z","iopub.execute_input":"2022-04-12T21:01:44.223645Z","iopub.status.idle":"2022-04-12T21:01:44.234512Z","shell.execute_reply.started":"2022-04-12T21:01:44.223617Z","shell.execute_reply":"2022-04-12T21:01:44.233500Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"#k-Nearest Neighbour Imputation\n\nknn_imputer = KNNImputer(n_neighbors = 3)\n\nX = np.round(knn_imputer.fit_transform(df_missing))\n#Rounding them because these are categorical features\n\ndf_missing = pd.DataFrame(X, columns = df_missing.columns)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:44.236105Z","iopub.execute_input":"2022-04-12T21:01:44.236731Z","iopub.status.idle":"2022-04-12T21:01:59.848313Z","shell.execute_reply.started":"2022-04-12T21:01:44.236660Z","shell.execute_reply":"2022-04-12T21:01:59.847304Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"#now lets join both dataframes \n\ndf_pre2 = pd.merge(df_missing, df_non_missing, on = 'enrollee_id')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:59.849738Z","iopub.execute_input":"2022-04-12T21:01:59.850032Z","iopub.status.idle":"2022-04-12T21:01:59.875132Z","shell.execute_reply.started":"2022-04-12T21:01:59.850003Z","shell.execute_reply":"2022-04-12T21:01:59.874068Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"If you remember i did concatenation between train and test data before preprocessing. Now after preprocessing of data we can seprate train and test data","metadata":{}},{"cell_type":"code","source":"train = df_pre2[df_pre2['target'] != -1].reset_index(drop = True)\ntest = df_pre2[df_pre2['target'] == -1].reset_index(drop = True)\n\nX = train.drop(['enrollee_id', 'target'], axis = 1)\nY = train['target']\n\n# drop fake target feature from test data \ntest = test.drop('target', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:59.876534Z","iopub.execute_input":"2022-04-12T21:01:59.876867Z","iopub.status.idle":"2022-04-12T21:01:59.894210Z","shell.execute_reply.started":"2022-04-12T21:01:59.876837Z","shell.execute_reply":"2022-04-12T21:01:59.893134Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Metrics <a id = \"4\" ></a>\n\nI think before selecting an optimal model for given data first we have to analayze target feature. Target Feature can be discrete in case of classification problem or continuous in case of Regression Problem\n\nIf we talk briefly about classification problems, the most common metrics used are:\n\n\n- **[Accuracy](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : It is one of the most straightforward metrics used in machine learning. It defines how accurate your model is. For example, if you build a model that classifies 90 samples accurately, your accuracy is 90% or 0.90. If only 83 samples are classified correctly, the accuracy of your model is 83% or 0.83. Simple.\n             \n     **[Scikit-learn user guide for accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)**\n     \n\n- **[Precision](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** :  Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good\n\n     **[True Positives (TP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted  class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n\n     **[True Negatives (TN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n\n     False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n\n     **[False Positives (FP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** – When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n\n     **[False Negatives (FN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** – When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n\n     **[Scikit-learn user guide for Precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)**\n     \n     ![](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)     \n     \n\n- **[Recall(Sensitivity)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? We have got recall of 0.631 which is good for this model as it’s above 0.5.\n\n     **[Scikit-learn user guide for Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)**\n\n\n- **[Confusion Matrix](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)** : A much better way to evaluate the performance of a classifier is to look at the confusion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.\n\n    **[Scikit-Learn user guide for Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)**\n     \n     \n- **[F1 score (F1)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701. \n\n     **[Scikit-learn user guide for F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)**\n\n\n- **[Area under the ROC (Receiver Operating Characteristic) curve](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)** : AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.\n\n     **[Scikit-learn user guide for AUC under the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)**\n\n\n\n\n\n\n**When it comes to regression, the most commonly used [evaluation metrics](https://towardsdatascience.com/evaluation-metrics-model-selection-in-linear-regression-73c7573208be) are:**\n\n\n- Mean absolute error (MAE)\n- Mean squared error (MSE)\n- Root mean squared error (RMSE)\n- Root mean squared logarithmic error (RMSLE)\n- Mean percentage error (MPE)\n- Mean absolute percentage error (MAPE)\n- R2\n\n **[Scikit-learn user guide for regression evaluation metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)**","metadata":{}},{"cell_type":"code","source":"sns.countplot(train['target'], edgecolor = 'black')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:59.895795Z","iopub.execute_input":"2022-04-12T21:01:59.896502Z","iopub.status.idle":"2022-04-12T21:02:00.039380Z","shell.execute_reply.started":"2022-04-12T21:01:59.896456Z","shell.execute_reply":"2022-04-12T21:02:00.038229Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"We see that the target is **skewed** and thus the best metric for this binary classification problem would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset.","metadata":{}},{"cell_type":"markdown","source":"# Model <a id = \"5\"></a>\n\nIn this notebook i would like to use [Extreme Gradient Boosting (XGBoost)](https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/) Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2 ,random_state = 42)\n\nclf = XGBClassifier()\n\nclf.fit(X_train, y_train)\n\ny_train_pred = clf.predict_proba(X_train)\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = clf.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")\n\nfpr, tpr, _ = roc_curve(y_val, y_val_pred_pos)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:00.041063Z","iopub.execute_input":"2022-04-12T21:02:00.041788Z","iopub.status.idle":"2022-04-12T21:02:00.750695Z","shell.execute_reply.started":"2022-04-12T21:02:00.041732Z","shell.execute_reply":"2022-04-12T21:02:00.749850Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"As we can see model is overfitting the data, we can do various things to resolve this problem like we can increase data set size in balanced manner and we can also tune hyperparameters of model ","metadata":{}},{"cell_type":"markdown","source":"Let's plot AUC Curve","metadata":{}},{"cell_type":"code","source":"def plot_auc_curve(fpr, tpr, auc):\n    plt.figure(figsize = (16,6))\n    plt.plot(fpr,tpr,'b+',linestyle = '-')\n    plt.fill_between(fpr, tpr, alpha = 0.5)\n    plt.ylabel('True Postive Rate')\n    plt.xlabel('False Postive Rate')\n    plt.title(f'ROC Curve Having AUC = {auc}')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:00.751947Z","iopub.execute_input":"2022-04-12T21:02:00.752415Z","iopub.status.idle":"2022-04-12T21:02:00.759544Z","shell.execute_reply.started":"2022-04-12T21:02:00.752380Z","shell.execute_reply":"2022-04-12T21:02:00.758642Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"plot_auc_curve(fpr, tpr, auc_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:00.761031Z","iopub.execute_input":"2022-04-12T21:02:00.761750Z","iopub.status.idle":"2022-04-12T21:02:01.104774Z","shell.execute_reply.started":"2022-04-12T21:02:00.761708Z","shell.execute_reply":"2022-04-12T21:02:01.103909Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"# Learning Curve <a id = \"6\"></a>\n\n\nTraining 3 examples will easily have 0 errors because we can always find any curve that exactly touches 3 points.\n\n* As the training set gets larger, the error for a function increases. \n* The error value will plateau out after a certain m, or training set size.\n\n\n**With high bias**\n\n\n* Low training set size: causes training cost to be low and cross validation cost to be high\n* Large training set size: causes both training cost and cross validation cost to be high with training cost = cross validation cost\n\n\n![img1](https://www.dataquest.io/wp-content/uploads/2019/01/low_high_var.png)\n\n**If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.**\n\n\nFor high variance, we have the following relationships in terms of the training set size:\n\n\n**With high variance**\n\n\n* Low training set size: training cost will be low and cross validation cost will be high\n* Large training set size: training cost increases with training set size and cross validation cost decreases without leveling off. Also, training cost < cross   validation cost but the difference between them remains significant.\n\n**If a learning algorithm is suffering from high variance, getting more training data is likely to help.**\n\nYou can visit below given link for more detailed intuition\n\n[Learning Curves for machine learning](https://www.dataquest.io/blog/learning-curves-machine-learning/)","metadata":{}},{"cell_type":"code","source":"# funtion to plot learning curves\n\ndef plot_learning_cuve(model, X, Y):\n    \n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 11)\n    train_loss, test_loss = [], []\n    \n    for m in range(200,len(x_train),200):\n        \n        model.fit(x_train.iloc[:m,:], y_train[:m])\n        y_train_prob_pred = model.predict_proba(x_train.iloc[:m,:])\n        train_loss.append(log_loss(y_train[:m], y_train_prob_pred))\n        \n        y_test_prob_pred = model.predict_proba(x_test)\n        test_loss.append(log_loss(y_test, y_test_prob_pred))\n        \n    plt.figure(figsize = (15,8))\n    plt.plot(train_loss, 'r-+', label = 'Training Loss')\n    plt.plot(test_loss, 'b-', label = 'Test Loss')\n    plt.xlabel('Number Of Batches')\n    plt.ylabel('Log-Loss')\n    plt.legend(loc = 'best')\n\n\n\n    plt.show()\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:01.106184Z","iopub.execute_input":"2022-04-12T21:02:01.106488Z","iopub.status.idle":"2022-04-12T21:02:01.117835Z","shell.execute_reply.started":"2022-04-12T21:02:01.106459Z","shell.execute_reply":"2022-04-12T21:02:01.116805Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"plot_learning_cuve(XGBClassifier(), X, Y)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:01.119304Z","iopub.execute_input":"2022-04-12T21:02:01.119699Z","iopub.status.idle":"2022-04-12T21:02:30.785768Z","shell.execute_reply.started":"2022-04-12T21:02:01.119617Z","shell.execute_reply":"2022-04-12T21:02:30.784795Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"It's a high variance problem ","metadata":{}},{"cell_type":"code","source":"sns.countplot(Y, edgecolor = 'black')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:30.787118Z","iopub.execute_input":"2022-04-12T21:02:30.787397Z","iopub.status.idle":"2022-04-12T21:02:30.926195Z","shell.execute_reply.started":"2022-04-12T21:02:30.787370Z","shell.execute_reply":"2022-04-12T21:02:30.925258Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"Let's try to increase data in balanced manner using Synthetic Minority Oversampling Technique (SMOTE) ","metadata":{}},{"cell_type":"markdown","source":"# Oversampling using SMOTE <a id = \"7\"></a>\n\n[SMOTE for Imbalanced Classification](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\\\n[SMOTE implementation](https://imbalanced-learn.org/stable/generated/imblearn.over_sampling.SMOTE.html)","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:30.927692Z","iopub.execute_input":"2022-04-12T21:02:30.927990Z","iopub.status.idle":"2022-04-12T21:02:30.931692Z","shell.execute_reply.started":"2022-04-12T21:02:30.927962Z","shell.execute_reply":"2022-04-12T21:02:30.930984Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"smote = SMOTE(random_state = 402)\nX_smote, Y_smote = smote.fit_resample(X,Y)\n\n\nsns.countplot(Y_smote, edgecolor = 'black')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:30.932654Z","iopub.execute_input":"2022-04-12T21:02:30.933084Z","iopub.status.idle":"2022-04-12T21:02:31.150976Z","shell.execute_reply.started":"2022-04-12T21:02:30.933055Z","shell.execute_reply":"2022-04-12T21:02:31.150149Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"print(X_smote.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:31.152084Z","iopub.execute_input":"2022-04-12T21:02:31.152485Z","iopub.status.idle":"2022-04-12T21:02:31.156625Z","shell.execute_reply.started":"2022-04-12T21:02:31.152456Z","shell.execute_reply":"2022-04-12T21:02:31.155886Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_smote, Y_smote, test_size = 0.2 ,random_state = 42)\n\nclf = XGBClassifier()\n\nclf.fit(X_train, y_train)\n\ny_train_pred = clf.predict_proba(X_train)\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = clf.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:31.157702Z","iopub.execute_input":"2022-04-12T21:02:31.158102Z","iopub.status.idle":"2022-04-12T21:02:32.330194Z","shell.execute_reply.started":"2022-04-12T21:02:31.158074Z","shell.execute_reply":"2022-04-12T21:02:32.329271Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"plot_learning_cuve(XGBClassifier(), X_smote, Y_smote)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:02:32.331703Z","iopub.execute_input":"2022-04-12T21:02:32.332331Z","iopub.status.idle":"2022-04-12T21:03:45.094812Z","shell.execute_reply.started":"2022-04-12T21:02:32.332290Z","shell.execute_reply":"2022-04-12T21:03:45.093752Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"Let's try to increase more data to conquer overfitting using SMOTE","metadata":{}},{"cell_type":"code","source":"smote = SMOTE(random_state = 446)\nX_smote1, Y_smote1 = smote.fit_resample(X,Y)\n\n\nX_final = pd.concat([X_smote, X_smote1], axis = 0).reset_index(drop = True)\nY_final = pd.concat([Y_smote, Y_smote1], axis = 0).reset_index(drop = True)\n\nsns.countplot(Y_final, edgecolor = 'black')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:03:45.096504Z","iopub.execute_input":"2022-04-12T21:03:45.097155Z","iopub.status.idle":"2022-04-12T21:03:45.335846Z","shell.execute_reply.started":"2022-04-12T21:03:45.097106Z","shell.execute_reply":"2022-04-12T21:03:45.334877Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"print(X_final.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:03:45.337382Z","iopub.execute_input":"2022-04-12T21:03:45.337734Z","iopub.status.idle":"2022-04-12T21:03:45.342906Z","shell.execute_reply.started":"2022-04-12T21:03:45.337702Z","shell.execute_reply":"2022-04-12T21:03:45.341961Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_final, Y_final, test_size = 0.2 ,random_state = 42)\n\nclf = XGBClassifier()\n\n\nclf.fit(X_train, y_train)\n\ny_train_pred = clf.predict_proba(X_train)\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = clf.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:03:45.343999Z","iopub.execute_input":"2022-04-12T21:03:45.344277Z","iopub.status.idle":"2022-04-12T21:03:47.797002Z","shell.execute_reply.started":"2022-04-12T21:03:45.344251Z","shell.execute_reply":"2022-04-12T21:03:47.793914Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"plot_learning_cuve(XGBClassifier(), X_final, Y_final)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:03:47.798435Z","iopub.execute_input":"2022-04-12T21:03:47.798799Z","iopub.status.idle":"2022-04-12T21:08:25.077607Z","shell.execute_reply.started":"2022-04-12T21:03:47.798764Z","shell.execute_reply":"2022-04-12T21:08:25.076509Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"Now as we can see gap decreases that mean we are going good but this was only for illustration in further notebook i will use smote only once because using it twice may change distribution of data too much","metadata":{}},{"cell_type":"markdown","source":"Bayesian optimization algorithm need a function they can optimize. Most of the time, it’s about the minimization of this function, like we minimize loss.","metadata":{}},{"cell_type":"code","source":"def optimize(params, param_names, x, y):\n   \n\n    # convert params to dictionary\n    params = dict(zip(param_names, params))\n\n    # initialize model with current parameters\n    clf = XGBClassifier(tree_method = 'hist', **params)\n    \n    # initialize stratified k fold\n    kf = StratifiedKFold(n_splits = 5)\n    \n    i = 0\n    \n    # initialize auc scores list\n    auc_scores = []\n    \n    #loop over all folds\n    for index in kf.split(X = x, y = y):\n        train_index, test_index = index[0], index[1]\n        \n        \n        \n        x_train = x.iloc[train_index,:]\n        y_train = y[train_index]\n\n        smote = SMOTE(random_state = 446)\n        x_train, y_train = smote.fit_resample(x_train,y_train)\n        \n        x_test = x.iloc[test_index,:]\n        y_test = y[test_index]\n        \n        #fit model\n        clf.fit(x_train, y_train)\n        \n        y_pred = clf.predict_proba(x_test)\n        y_pred_pos = y_pred[:,1]\n        \n        auc = roc_auc_score(y_test, y_pred_pos)\n        print(f'Current parameters of fold number {i} -> {params}')\n        print(f'AUC score of test {i} f {auc}')\n\n        i = i+1\n        auc_scores.append(auc)\n        \n    return -1 * np.mean(auc_scores)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:08:25.079573Z","iopub.execute_input":"2022-04-12T21:08:25.080034Z","iopub.status.idle":"2022-04-12T21:08:25.093235Z","shell.execute_reply.started":"2022-04-12T21:08:25.079990Z","shell.execute_reply":"2022-04-12T21:08:25.092246Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"markdown","source":"So, let’s say, you want to find the best parameters for best accuracy and obviously, the more the accuracy is better. Now we cannot minimize the accuracy, but we can minimize it when we multiply it by -1. This way, we are minimizing the negative of accuracy, but in fact, we are maximizing accuracy. Using Bayesian optimization with gaussian process can be accomplished by using [gp_minimize function from scikit-optimize (skopt) library](https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html). Let’s take a look at how we can tune the parameters of our xgboost model using this\nfunction.\n\n[Parameters for XGBoost Classifier](https://xgboost.readthedocs.io/en/latest/parameter.html)\n\nI am try to optimize the model with 4 hyperparameters but you can try many more given in above mentioned link","metadata":{}},{"cell_type":"code","source":"#define a parameter space\n\nparam_spaces = [space.Integer(100, 2000, name = 'n_estimators'),\n                space.Real(0.01,100, name = 'min_child_weight'),\n                space.Real(0.01,1000, name = 'gamma'),\n                space.Real(0.1, 1, prior = 'uniform', name = 'colsample_bytree'),\n]\n\n# make a list of param names this has to be same order as the search space inside the main function\nparam_names = ['n_estimators' ,'min_child_weight', 'gamma', 'colsample_bytree']\n\n# by using functools partial, i am creating a new function which has same parameters as the optimize function except \n# for the fact that only one param, i.e. the \"params\" parameter is required. \n# This is how gp_minimize expects the optimization function to be. \n# You can get rid of this by reading data inside the optimize function or by defining the optimize function here.\n\noptimize_function = partial(optimize, param_names = param_names, x = X, y = Y)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:08:25.099503Z","iopub.execute_input":"2022-04-12T21:08:25.099829Z","iopub.status.idle":"2022-04-12T21:08:25.113499Z","shell.execute_reply.started":"2022-04-12T21:08:25.099800Z","shell.execute_reply":"2022-04-12T21:08:25.112324Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"# output of this cell is very large that's why it is hidden\n\nresult = gp_minimize(optimize_function, dimensions = param_spaces, n_calls = 20, n_random_starts = 5, verbose = 10)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-04-12T21:08:25.114760Z","iopub.execute_input":"2022-04-12T21:08:25.115178Z","iopub.status.idle":"2022-04-12T21:13:21.135786Z","shell.execute_reply.started":"2022-04-12T21:08:25.115148Z","shell.execute_reply":"2022-04-12T21:13:21.134821Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"best_params = dict(zip(param_names, result.x))\nprint(f'Best Parameters : {best_params}')\nprint(f'Best AUC score : {result.fun}')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:13:21.137777Z","iopub.execute_input":"2022-04-12T21:13:21.138524Z","iopub.status.idle":"2022-04-12T21:13:21.146501Z","shell.execute_reply.started":"2022-04-12T21:13:21.138475Z","shell.execute_reply":"2022-04-12T21:13:21.145311Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"let's again plot learning cuve with hyperparameters this time","metadata":{}},{"cell_type":"code","source":"# splitting train and validation data\n\nX_train, X_val, y_train, y_val = train_test_split(X,Y, test_size = 0.2, random_state = 24)\n\nsmote = SMOTE(random_state = 446)\nX_train, y_train = smote.fit_resample(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:13:21.148541Z","iopub.execute_input":"2022-04-12T21:13:21.148996Z","iopub.status.idle":"2022-04-12T21:13:21.217470Z","shell.execute_reply.started":"2022-04-12T21:13:21.148953Z","shell.execute_reply":"2022-04-12T21:13:21.216689Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"# initialize model with best parameters\nclf = XGBClassifier(**best_params)\n\n# fit model\nclf.fit(X_train, y_train)\n\n# predicting probabilities of training data\ny_train_pred = clf.predict_proba(X_train)\n\n\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = clf.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")\n\nfpr, tpr, _ = roc_curve(y_val, y_val_pred_pos)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:13:21.219867Z","iopub.execute_input":"2022-04-12T21:13:21.220296Z","iopub.status.idle":"2022-04-12T21:13:22.259423Z","shell.execute_reply.started":"2022-04-12T21:13:21.220252Z","shell.execute_reply":"2022-04-12T21:13:22.258393Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"plot_learning_cuve(XGBClassifier(**best_params),X_smote,Y_smote)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:13:22.260882Z","iopub.execute_input":"2022-04-12T21:13:22.261460Z","iopub.status.idle":"2022-04-12T21:14:28.684094Z","shell.execute_reply.started":"2022-04-12T21:13:22.261418Z","shell.execute_reply":"2022-04-12T21:14:28.683040Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"plot_auc_curve(fpr, tpr, auc_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:14:28.685636Z","iopub.execute_input":"2022-04-12T21:14:28.686234Z","iopub.status.idle":"2022-04-12T21:14:28.980284Z","shell.execute_reply.started":"2022-04-12T21:14:28.686189Z","shell.execute_reply":"2022-04-12T21:14:28.979305Z"},"trusted":true},"execution_count":146,"outputs":[]}]}